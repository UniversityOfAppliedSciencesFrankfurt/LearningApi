################################################################################
# This .gitignore file was automatically created by Microsoft(R) Visual Studio.
################################################################################
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.

# User-specific files
*.suo
*.user
*.sln.docstates
*.vs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
x64/
build/   public override IScore Run(double[][] data, IContext ctx)
        {
            double loss = 0;
            RbmScore score = new RbmScore();

            //if(m_WriteLossToFile)
            using (StreamWriter sw = new StreamWriter(File.Open("delta - gradient.csv", FileMode.Create)))
            {
                int[] indices = new int[data.Length];
                for (int i = 0; i < indices.Length; ++i)
                    indices[i] = i;

                int epoch = 0;
                while (epoch < maxEpochs)
                {
                    loss = 0;
                    MathFunctions.Shuffle(indices);

                    // 
                    // Traversing through shuffled data.
                    for (int idx = 0; idx < indices.Length; ++idx)
                    {
                        //Debug.WriteLine($"---- epoch: {epoch} ---{idx} of {indices.Length} ------");

                        int i = indices[idx];  // i points to curr train data

                        //
                        // Copy train data to visible values.
                        // It copies input vector to member m_VisibleValues
                        for (int j = 0; j < numVisible; ++j)
                            m_VisibleValues[j] = data[i][j];

                        //
                        // Compute hidden node values for current input vector.
                        for (int hiddenIndx = 0; hiddenIndx < numHidden; ++hiddenIndx)
                        {
                            double sum = 0.0;

                            for (int visibleIndx = 0; visibleIndx < numVisible; ++visibleIndx)
                                sum += m_VisibleValues[visibleIndx] * vhWeights[visibleIndx][hiddenIndx];

                            sum += hidBiases[hiddenIndx]; // add the hidden bias
                                                          //hidProbs[hiddenIndx] = m_ActivationFunction(sum); // compute prob of h activation
                                                          //double pr = m_Rnd.NextDouble();  // determine 0/1 h node value
                                                          //if (hidProbs[hiddenIndx] > pr)
                                                          //    hidValues[hiddenIndx] = 1;
                                                          //else
                                                          //    hidValues[hiddenIndx] = 0;

                            var sumPrime = m_ActivationFunction(sum);

                            //if (sumPrime > 0.5)
                            //    hidValues[hiddenIndx] = 1;
                            //else
                            //    hidValues[hiddenIndx] = 0;

                            double pr = m_Rnd.NextDouble();  // determine 0/1 h node value
                            if (sumPrime > pr)
                                hidValues[hiddenIndx] = 1;
                            else
                                hidValues[hiddenIndx] = 0;
                        }

                        // compute positive gradient =  outer product of v & h
                        double[][] posGrad = MathFunctions.OuterProd(m_VisibleValues, hidValues);

                        // reconstruct visual Nodes as v'
                        double[] vPrime = new double[numVisible];  // v' in Wikipedia
                        for (int v = 0; v < numVisible; ++v)
                        {
                            double sum = 0.0;
                            for (int h = 0; h < numHidden; ++h)
                                sum += hidValues[h] * vhWeights[v][h];
                            sum += visBiases[v]; // add visible bias
                            double probActiv = m_ActivationFunction(sum);
                            double pr = m_Rnd.NextDouble();
                            if (probActiv > pr)
                                vPrime[v] = 1;
                            else
                                vPrime[v] = 0;
                        }

                        //
                        // Compute new hidden Nodes as h', using v'
                        double[] hPrime = new double[numHidden];
                        for (int hiddenValIndx = 0; hiddenValIndx < numHidden; ++hiddenValIndx)
                        {
                            double sum = 0.0;
                            for (int v = 0; v < numVisible; ++v)
                                sum += vPrime[v] * vhWeights[v][hiddenValIndx];
                            sum += hidBiases[hiddenValIndx]; // add the hidden bias
                            double probActiv = m_ActivationFunction(sum); // apply activation
                            double pr = m_Rnd.NextDouble();  // determine 0/1 node value
                            if (probActiv > pr)
                                hPrime[hiddenValIndx] = 1;
                            else
                                hPrime[hiddenValIndx] = 0;
                        }

                        //printVector("Weights -1", vhWeights);

                        //
                        // Compute negative grad using v' and h'
                        double[][] negGrad = MathFunctions.OuterProd(vPrime, hPrime);

#if DEBUG
                        var val = calcDelta(posGrad, negGrad);
                        loss += val / (numHidden * numVisible);
#endif
                        // printVector("PosGrad", posGrad);
                        // printVector("NegGrad", negGrad);

                        // Update weights
                        for (int row = 0; row < numVisible; ++row)
                            for (int col = 0; col < numHidden; ++col)
                                vhWeights[row][col] += learnRate * (posGrad[row][col] - negGrad[row][col]);

                        // Update visBiases
                        for (int v = 0; v < numVisible; ++v)
                            visBiases[v] += learnRate * (m_VisibleValues[v] - vPrime[v]);
                        // update hidBiases
                        for (int h = 0; h < numHidden; ++h)
                            hidBiases[h] += learnRate * (hidValues[h] - hPrime[h]);

                        // Include these lines to print out internal result.
                        //printVector("Visible", m_VisibleValues);
                        //printVector("Hidden", hidValues);
                        //printVector("Weights", vhWeights);
                    }

                    sw.WriteLine($"{loss / indices.Length};{epoch}");
                    // Loss of iteration is calculated as 
                    // SUM(posGrad-negGrad) / number of training vectors
                    Debug.WriteLine($"loss: {loss / indices.Length}");

                    score.Loss = loss / indices.Length;

                    ++epoch;
                }
            }

            score.HiddenValues = new List<double>(this.hidValues).ToArray();
            score.HiddenBisases = new List<double>(this.hidBiases).ToArray();
            score.Weights = new List<double[]>(this.vhWeights).ToArray();

            return score;
        }
bld/
[Bb]in/
[Oo]bj/

# Roslyn cache directories
*.ide/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

#NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

*_i.c
*_p.c
*_i.h
*.ilk
*.meta
*.obj
*.pch
*.pdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opensdf
*.sdf
*.cachefile

# Visual Studio profiler
*.psess
*.vsp
*.vspx

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding addin-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# NCrunch
_NCrunch_*
.*crunch*.local.xml

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
## TODO: Comment the next line if you want to checkin your
## web deploy settings but do note that will include unencrypted
## passwords
#*.pubxml

# NuGet Packages Directory
packages/*
## TODO: If the tool you use requires repositories.config
## uncomment the next line
#!packages/repositories.config

# Enable "build/" folder in the NuGet Packages folder since
# NuGet packages use it for MSBuild targets.
# This line needs to be after the ignore of the build folder
# (and the packages folder if the line above has been uncommented)
!packages/build/

# Windows Azure Build Output
csx/
*.build.csdef

# Windows Store app package directory
AppPackages/

# Others
sql/
*.Cache
ClientBin/
[Ss]tyle[Cc]op.*
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.pfx
*.publishsettings
node_modules/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm

# SQL Server files
*.mdf
*.ldf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings

# Microsoft Fakes
FakesAssemblies/


/LearningApi/src/DataProviders/CsvDataProvider/bin/Debug
/LearningApi/src/DataProviders/CsvDataProvider/obj/Debug
/LearningApi/src/MLAlgorithms/AnomDetect.KMeans/bin/
/LearningApi/src/MLAlgorithms/AnomDetect.KMeans/obj/
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNet.BackPropagation/bin/
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNet.BackPropagation/obj/
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNetworks.Core/bin/
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNetworks.Core/obj/
/LearningApi/src/LearningApi/bin
/LearningApi/src/LearningApi/obj
/LearningApi/LearningApi/.vs
/LearningApi/src/LearningApi/.vs
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNetworks.Core/obj
/LearningApi/src/MLAlgorithms/NeuralNetworks/NeuralNetworks.Core/bin
/LearningApi/LearningApi/test/UnitTests/obj
/LearningApi/LearningApi/test/UnitTests/bin

/LearningApi/.vs/LearningApi/v14
LearningApi/test/bin/
LearningApi/test/obj/
LearningApi/test/bin/*
LearningApi/test/obj/*

/LearningApi/.vs
/LearningApi/test/obj
/LearningApi/test/bin
/LearningApi/src/Normalizers/MinMaxNormalizer/bin/*
/LearningApi/src/Normalizers/MinMaxNormalizer/obj/*
/LearningApi/src/Normalizers/GaussNormalizer/bin/*
/LearningApi/src/Normalizers/GaussNormalizer/obj/*
/LearningApi/src/CannyEdgeDetector/obj/*
/LearningApi/src/CannyEdgeDetector/bin/*
/LearningApi/test/scaling features/~$precalculated_samples.xlsx
/LearningApi/test/sample_data/iris/~$iris.xlsx
/LearningApi/src/MLAlgorithms/LogisticRegression/bin/Debug/netstandard1.2
/LearningApi/packages/ZedGraph.5.1.7
/LearningApi/MouseGestureRecognition/.vs/MouseGestureRecognition/v15/Server/sqlite3
/.vs
/LearningApi/TestRunner/Properties/launchSettings.json
/LearningApi/packages
